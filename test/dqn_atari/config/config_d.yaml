# Create config.yaml file to overwrite default settings

# General settings
loglevel: info

# Redis settings
redis_address: "redis"

local:

# Environment information
env:
  vectorize: False  # Atari wrappers do not support vectorized environments
  name: ALE/Pong-v5
  kwargs:
    frameskip: 1
  obs_shape: [1, 84, 84]
  n_actions: 6
  wrappers:
    AtariPreprocessing:
      kwargs:
    AtariExpandImage:
      kwargs:

max_env_steps: 10_000_000

# Algorithm choice
algorithm: "dqn"

# DQN parameters
dqn:
  batch_size: 128
  train_epochs: 25
  update_samples: 50
  fill_buffer: False
  action_masking: False
  min_samples: null
  max_model_delay: 10

  agent:
    type: DistributionalDQNAgent
    kwargs:
      lr: 0.0001
      gamma: 0.99
      multistep: 3
      grad_clip: 5.
      q_clip: 50
      tau: 0.001  # For soft target update
      device: "cpu"

  network:
    type: "CNNDistributionalDQN" # "CNNDistributionalDQN"
    kwargs:
      input_shape: [1, 84, 84]
      output_dims: 6
      n_quantiles: 32

  replay_buffer:
    type: "ReplayBuffer"
    kwargs:
      max_size: 500_000
  
  observation_transform:
    type: "ImageNormalization"
    kwargs:
  
  action_transform:
    type: "Choice"
    kwargs:
      prob: [0.95, 0.05]
      transforms:
        - type: "Identity"
        - type: "ReplaceWithNoise"
          kwargs:
            noise:
              type: "UniformDiscreteNoise"
              kwargs:
                size_n: 6

# Training node settings
checkpoint:
  save: False
  epochs: 100  # Checkpoint every 100th iteration
  save_buffer: False
  load: False
  load_buffer: False
  load_config: False

# Telemetry node settings
telemetry:
  update_interval: 5
  moving_average: 20
  transforms:
    - type: "MetricByKey"
      kwargs:
        key: "ep_reward"
        name: "rewards"
    - type: "MetricByKey"
      kwargs:
        key: "ep_steps"
        name: "steps"
    - type: "MetricByKey"
      kwargs:
        key: "total_steps"
        name: "n_env_steps"
    - type: "CompareValue"
      kwargs:
        key: "ep_reward"
        name: "wins"
        value: 200
        op: gt  # Greater than
    - type: "Timer"
  log_keys:
    - "rewards_av"
    - "steps_av"
  callbacks:
    - type: "SaveBest"
      kwargs:
        key: "rewards_av"
        channel: "save_best"

monitoring:
  file_storage:
    path: "/home/SoulsAI/saves"
    plot:
      xkey: "n_env_steps"
      ykeys:
        - "rewards_av"
        - "steps_av"
        - "wins_av"
        - "time"
  grafana: True
  prometheus: True
  wandb:
    project: "soulsai_dqn_atari"
    entity: "amacati"
    group: "testing"
    save_dir: "/home/SoulsAI/saves"

# Client settings
max_episodes: -1  # -1 for infinite runs
enable_interrupt: False
step_delay: 0.003

watchdog:
  enable: False
  minimum_samples: -1