# Create config.yaml file to overwrite default settings

# General settings
loglevel: info

local:

# Redis settings
redis_address: "redis"

# Environment information
env:
  name: LunarLander-v2
  vectorize: False
  kwargs:
  obs_shape: [8]
  n_actions: 4
  wrappers:

max_env_steps: 100_000

# Algorithm choice
algorithm: "dqn"

# DQN parameters
dqn:
  batch_size: 64
  train_epochs: 25
  update_samples: 25
  fill_buffer: False
  action_masking: False
  min_samples: null
  max_model_delay: 10

  agent:
    type: DistributionalDQNAgent
    kwargs:
      lr: 0.001
      gamma: 0.99
      multistep: 4
      grad_clip: 1.
      q_clip: 300
      tau: 0.001  # For soft target update
      device: "cuda"
  
  network:
    type: "DistributionalDQN"
    kwargs:
      input_dims: 8
      output_dims: 4
      layer_dims: 128
  
  replay_buffer:
    type: "ReplayBuffer"
    kwargs:
      max_size: 100_000
  
  observation_transform:
    type: "Normalize"
    kwargs:
      keys:
        - "obs"
      shapes:
        - [8]
  
  action_transform:
    type: "Choice"
    kwargs:
      prob: [0.9, 0.1]
      transforms:
        - type: "Identity"
        - type: "ReplaceWithNoise"
          kwargs:
            noise:
              type: "UniformDiscreteNoise"
              kwargs:
                size_n: 4

# Training node settings
checkpoint:
  save: False
  epochs: 100  # Checkpoint every 100th iteration
  save_buffer: False
  load: False
  load_buffer: False
  load_config: False

# Telemetry node settings
telemetry:
  update_interval: 5
  moving_average: 20
  transforms:
    - type: "MetricByKey"
      kwargs:
        key: "ep_reward"
        name: "rewards"
    - type: "MetricByKey"
      kwargs:
        key: "ep_steps"
        name: "steps"
    - type: "MetricByKey"
      kwargs:
        key: "total_steps"
        name: "n_env_steps"
    - type: "CompareValue"
      kwargs:
        key: "ep_reward"
        name: "wins"
        value: 200
        op: gt  # Greater than
    - type: "Timer"
  log_keys:
    - "rewards_av"
    - "steps_av"
  callbacks:
    - type: "SaveBest"
      kwargs:
        key: "rewards_av"
        channel: "save_best"

monitoring:
  file_storage:
    path: "/home/SoulsAI/saves"
    plot:
      xkey: "n_env_steps"
      ykeys:
        - "rewards_av"
        - "steps_av"
        - "wins_av"
        - "time"
  grafana: True
  prometheus: True
  wandb:
    project: "soulsai_dqn"
    entity: "amacati"
    group: "testing"
    save_dir: "/home/SoulsAI/saves"

# Client settings
max_episodes: -1  # -1 for infinite runs
enable_interrupt: False
step_delay: 0.003

watchdog:
  enable: False
  minimum_samples: -1
