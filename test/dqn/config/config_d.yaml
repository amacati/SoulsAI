# Create config.yaml file to overwrite default settings

# General settings
loglevel: info
device: "cpu"

local:

# Redis settings
redis_address: "redis"

# Environment information
env:
  name: LunarLander-v2
  kwargs:
  obs_shape: [8]
  n_actions: 4
  wrappers:

max_env_steps: 100_000
gamma: 0.99

# Algorithm choice
algorithm: "dqn"

# DQN parameters
dqn:
  variant: distributional  # [vanilla, distributional]
  lr: 0.001
  eps_max: [0.1]
  eps_min: [0.1]
  eps_steps: [100_000]
  grad_clip: 5.
  q_clip: 200
  buffer_size: 100_000
  batch_size: 64
  train_epochs: 25
  update_samples: 25
  fill_buffer: False
  multistep: 4
  network:
    type: "DistributionalDQN"
    kwargs:
      input_dims: 8
      output_dims: 4
      layer_dims: 128
  replay_buffer:
    type: "ReplayBuffer"
    kwargs:
      max_size: 100_000
  observation_transform:
    type: "TensorNormalization"
    kwargs:
      key: "obs"
      shape: [8]
  action_transform:
    type: "Choice"
    kwargs:
      prob: [0.9, 0.1]
      transforms:
        - type: "Identity"
        - type: "ReplaceWithNoise"
          kwargs:
            noise:
              type: "UniformDiscreteNoise"
              kwargs:
                size_n: 4
  action_masking: False
  min_samples: null
  tau: 0.05  # For soft target update
  max_model_delay: 10

# Training node settings
checkpoint:
  save: False
  epochs: 100  # Checkpoint every 100th iteration
  save_buffer: False
  load: False
  load_buffer: False
  load_config: False

# Telemetry node settings
telemetry:
  update_interval: 5
  moving_average: 20
  transforms:
    - type: "MetricByKey"
      kwargs:
        key: "ep_reward"
        name: "rewards"
    - type: "MetricByKey"
      kwargs:
        key: "ep_steps"
        name: "steps"
    - type: "MetricByKey"
      kwargs:
        key: "total_steps"
        name: "n_env_steps"
    - type: "CompareValue"
      kwargs:
        key: "ep_reward"
        name: "wins"
        value: 200
        op: gt  # Greater than
    - type: "Timer"
  log_keys:
    - "rewards_av"
    - "steps_av"
  callbacks:
    - type: "SaveBest"
      kwargs:
        key: "rewards_av"
        channel: "save_best"

monitoring:
  file_storage:
    path: "/home/SoulsAI/saves"
    plot:
      xkey: "n_env_steps"
      ykeys:
        - "rewards_av"
        - "steps_av"
        - "wins_av"
        - "time"
  grafana: True
  prometheus: True
  wandb:
    project: "soulsai_dqn"
    entity: "amacati"
    group: "testing"
    save_dir: "/home/SoulsAI/saves"

# Client settings
max_episodes: -1  # -1 for infinite runs
enable_interrupt: False
step_delay: 0.003

watchdog:
  enable: False
  minimum_samples: -1
